# ğŸ“„ Retrieval-Augmented Generation (RAG) System with FastAPI, ChromaDB & Ollama

A lightweight yet powerful Retrieval-Augmented Generation (RAG) backend for semantic search and question answering over private documents.  
Built with **FastAPI**, **ChromaDB**, **SentenceTransformers**, and **Ollama LLM**.

---

## ğŸš€ Features
- **Upload PDFs & TXT files** for ingestion  
- PDF text extraction via **PyMuPDF**  
- Chunking with overlap for high-quality semantic search  
- **Local persistent vector storage** using ChromaDB  
- **SentenceTransformers (MiniLM-L6-v2)** for embeddings  
- **Ollama LLM** integration for fast and concise answers  
- **Optional spaCy NER anonymization** for sensitive info  
- Metadata filtering for scoped queries

---

## ğŸ›  Tech Stack
- [FastAPI](https://fastapi.tiangolo.com/) â€“ Modern Python API framework  
- [ChromaDB](https://docs.trychroma.com/) â€“ Vector database for embeddings  
- [SentenceTransformers](https://www.sbert.net/) â€“ State-of-the-art embeddings  
- [PyMuPDF](https://pymupdf.readthedocs.io/) â€“ PDF parsing & text extraction  
- [Ollama](https://ollama.com/) â€“ Local or cloud LLM inference engine  
- [spaCy](https://spacy.io/) â€“ Named entity recognition (optional)

---

## ğŸ“‚ Project Structure
â”œâ”€â”€ main_fastapi.py         # FastAPI backend with endpoints
â”œâ”€â”€ utils.py                # PDF/TXT processing, embeddings, helpers
â”œâ”€â”€ requirements.txt        # Python dependencies
â”œâ”€â”€ README.md               # Project documentation
â””â”€â”€ docs/
    â””â”€â”€ API.md              # separate API reference

---

## âš™ï¸ Installation

### Prerequisites
- Python **3.9+**
- [Ollama](https://ollama.com/)  accessible via API
- `pip` for installing dependencies


1ï¸âƒ£ Clone the repo
```bash
git clone https://github.com/your-username/your-rag-project.git
cd your-rag-project

2ï¸âƒ£ Install dependencies
bash
pip install -r requirements.txt

3ï¸âƒ£ Environment Variables
Create a .env file in your project directory:
OLLAMA_API_KEY=your_api_key_here
OLLAMA_MODEL = "deepseek-v3.1:671b-cloud"

4ï¸âƒ£ Run the API
uvicorn main_fastapi:app --reload
API available at:
http://127.0.0.1:8000

you can test it in:
http://127.0.0.1:8000/docs

